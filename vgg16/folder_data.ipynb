{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 8 classes.\n",
      "Found 0 images belonging to 8 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 5\n",
    "batch_size = 8\n",
    "num_epochs = 2\n",
    "image_size = (224,224)\n",
    "# ADD DATASET FROM DIRECTORY\n",
    "\n",
    "data_dir = r\"/mnt/c/Windows/System32/repos/thesis_raw_data/Doc_H-Data/\"\n",
    "\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "                             rotation_range=20,\n",
    "                             width_shift_range=0.1,\n",
    "                             height_shift_range=0.1,\n",
    "                             shear_range=0.2,\n",
    "                             zoom_range=0.2,\n",
    "                             horizontal_flip=True,\n",
    "                             validation_split=0.2,\n",
    "                             )\n",
    "\n",
    "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "train_ds = datagen.flow_from_directory(\n",
    "  data_dir,\n",
    "  classes='inferred',\n",
    "  class_mode='categorical',\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  target_size= image_size,\n",
    "  batch_size= batch_size)\n",
    "\n",
    "val_ds = val_datagen.flow_from_directory(\n",
    "  data_dir,\n",
    "  classes='inferred',\n",
    "  class_mode='categorical',\n",
    "  # validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  target_size= image_size,\n",
    "  batch_size= batch_size)\n",
    "\n",
    "\n",
    "val_ds.samples\n",
    "len(data_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# train_ds_ub = train_ds.unbatch()\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# images = list(train_ds_ub.map(lambda x, y: x))\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# labels = list(train_ds_ub.map(lambda x, y: y))\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[39m# print(labels)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate([y \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m train_ds], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(y)\n",
      "Cell \u001b[0;32mIn[37], line 7\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# train_ds_ub = train_ds.unbatch()\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# images = list(train_ds_ub.map(lambda x, y: x))\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# labels = list(train_ds_ub.map(lambda x, y: y))\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[39m# print(labels)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate([y \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m train_ds], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(y)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf-py38/lib/python3.8/site-packages/keras/preprocessing/image.py:156\u001b[0m, in \u001b[0;36mIterator.__next__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 156\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf-py38/lib/python3.8/site-packages/keras/preprocessing/image.py:165\u001b[0m, in \u001b[0;36mIterator.next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"For python 2.x.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \n\u001b[1;32m    161\u001b[0m \u001b[39mReturns:\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39m    The next batch.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlock:\n\u001b[0;32m--> 165\u001b[0m     index_array \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex_generator)\n\u001b[1;32m    166\u001b[0m \u001b[39m# The transformation of images is not under thread lock\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39m# so it can be done in parallel\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_batches_of_transformed_samples(index_array)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf-py38/lib/python3.8/site-packages/keras/preprocessing/image.py:134\u001b[0m, in \u001b[0;36mIterator._flow_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m     np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mseed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseed \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_batches_seen)\n\u001b[1;32m    133\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_index \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 134\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_index_array()\n\u001b[1;32m    136\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    137\u001b[0m     \u001b[39m# Avoiding modulo by zero error\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     current_index \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf-py38/lib/python3.8/site-packages/keras/preprocessing/image.py:97\u001b[0m, in \u001b[0;36mIterator._set_index_array\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_index_array\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 97\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex_array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn)\n\u001b[1;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshuffle:\n\u001b[1;32m     99\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex_array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mpermutation(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train_ds_ub = train_ds.unbatch()\n",
    "# images = list(train_ds_ub.map(lambda x, y: x))\n",
    "# labels = list(train_ds_ub.map(lambda x, y: y))\n",
    "\n",
    "# print(labels)\n",
    "\n",
    "y = np.concatenate([y for x, y in train_ds], axis=0)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_model = VGG16(include_top = False,\n",
    "            weights = 'imagenet', \n",
    "            input_tensor = None, \n",
    "            input_shape = (224,224,3), #shape of npy file data\n",
    "            pooling = None,\n",
    "            classes = 1000,\n",
    "            classifier_activation=\"softmax\") \n",
    "\n",
    "\n",
    "# Do not retrain convolutional layers\n",
    "for layer in vgg16_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "input_shape = keras.Input(shape=(224, 224, 3))\n",
    "\n",
    "# Add new fully connected layers\n",
    "x = Flatten()(vgg16_model.output)\n",
    "x = Dense(2048, activation='relu')(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "output = Dense(num_classes, activation='softmax')(x)  # num_classes is the number of classes in your dataset\n",
    "\n",
    "# Create a new model with the fully connected layers added\n",
    "model = Model(inputs=vgg16_model.input, outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Asked to retrieve element 0, but the Sequence has length 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Use the generators to train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train_ds,\n\u001b[1;32m      3\u001b[0m           validation_data \u001b[39m=\u001b[39;49m val_ds,\n\u001b[1;32m      4\u001b[0m           epochs\u001b[39m=\u001b[39;49mnum_epochs,\n\u001b[1;32m      5\u001b[0m           )\n",
      "File \u001b[0;32m~/miniconda3/envs/tf-py38/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/tf-py38/lib/python3.8/site-packages/keras/preprocessing/image.py:103\u001b[0m, in \u001b[0;36mIterator.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[1;32m    102\u001b[0m     \u001b[39mif\u001b[39;00m idx \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 103\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    104\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAsked to retrieve element \u001b[39m\u001b[39m{idx}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mbut the Sequence \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mhas length \u001b[39m\u001b[39m{length}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(idx\u001b[39m=\u001b[39midx, length\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m))\n\u001b[1;32m    107\u001b[0m         )\n\u001b[1;32m    108\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mseed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseed \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_batches_seen)\n",
      "\u001b[0;31mValueError\u001b[0m: Asked to retrieve element 0, but the Sequence has length 0"
     ]
    }
   ],
   "source": [
    "# Use the generators to train the model\n",
    "history = model.fit(train_ds,\n",
    "          validation_data = val_ds,\n",
    "          epochs=num_epochs,\n",
    "          )\n",
    "        #   steps_per_epoch= len(train_generator) // batch_size,\n",
    "        #   validation_data=test_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the trained model\n",
    "model.save('vgg16_trained.h5')\n",
    "\n",
    "# Plot the training and validation accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "print(history.history['val_accuracy'])\n",
    "#Find the x and y position of the highest test accuracy\n",
    "#list all val_accuracy values\n",
    "list = history.history['val_accuracy']\n",
    "\n",
    "#find highest val_accuracy\n",
    "ymax =  max(history.history['val_accuracy'])\n",
    "\n",
    "#find index of highest val_accuracy\n",
    "xpos = list.index(max(history.history['val_accuracy']))\n",
    "\n",
    "# Annotation for max accuracy\n",
    "plt.annotate('Max Accuracy @ {}%'.format(round(ymax*100,2)), xy=(xpos, ymax), xytext=(xpos, ymax+.05), ha = 'center', \n",
    "             arrowprops=dict(arrowstyle=\"->\", facecolor='black'))\n",
    "#Show plot             \n",
    "plt.show()\n",
    "# Plot the training and validation loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "print(history.history['val_accuracy'])\n",
    "#Find the x and y position of the highest test accuracy\n",
    "#list all val_accuracy values\n",
    "list = history.history['val_accuracy']\n",
    "\n",
    "#find highest val_accuracy\n",
    "ymax =  max(history.history['val_accuracy'])\n",
    "\n",
    "#find index of highest val_accuracy\n",
    "xpos = list.index(max(history.history['val_accuracy']))\n",
    "\n",
    "# Annotation for max accuracy\n",
    "plt.annotate('Max Accuracy @ {}%'.format(round(ymax*100,2)), xy=(xpos, ymax), xytext=(xpos, ymax+.05), ha = 'center', \n",
    "             arrowprops=dict(arrowstyle=\"->\", facecolor='black'))\n",
    "#Show plot             \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/54589669/confusion-matrix-error-classification-metrics-cant-handle-a-mix-of-multilabel\\\n",
    "for below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_batch = model.predict(val_ds)\n",
    "predicted_id= np.argmax(predicted_batch, axis=-1)\n",
    "\n",
    "print(predicted_id)\n",
    "\n",
    "true_id = tf.concat([y for x, y in val_ds], axis=0)\n",
    "true_id=np.argmax(true_id, axis=1)\n",
    "true_id[1]\n",
    "print(true_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "matrix = confusion_matrix(predicted_id, true_id)\n",
    "labels = [\"No Damage\", \"Slight Damage\", \"Moderate Damage\", \"Extensive Damage\", \"Completely Damaged\"]\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=labels)\n",
    "disp.plot(cmap=plt.cm.Oranges)\n",
    "plt.grid(None)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from sklearn import confusion_matrix\n",
    "# true_categories = tf.concat([y for x, y in val_ds], axis=0)\n",
    "\n",
    "# cm = confusion_matrix(true_categories, predicted_id)\n",
    "# fig = plt.figure(figsize = (8,8))\n",
    "# ax1 = fig.add_subplot(1,1,1)\n",
    "# sns.set(font_scale=1.4) #for label size\n",
    "# sns.heatmap(cm, annot=True, annot_kws={\"size\": 12},\n",
    "#      cbar = False, cmap='Purples');\n",
    "# ax1.set_ylabel('True Values',fontsize=14)\n",
    "# ax1.set_xlabel('Predicted Values',fontsize=14)\n",
    "# plt.show()\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "val_ds = np.argmax(val_ds, axis = -1)\n",
    "\n",
    "\n",
    "predictions = model.predict(train_ds)\n",
    "\n",
    "predictions = np.argmax(predictions)\n",
    "\n",
    "print(val_ds)\n",
    "print(predictions)\n",
    "# matrix = confusion_matrix(val_ds, predictions)\n",
    "# labels = [\"Non-collapse\", \"Partial Collapse\", \"Global Collapse\"]\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=labels)\n",
    "\n",
    "# disp.plot(cmap=plt.cm.Oranges)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
